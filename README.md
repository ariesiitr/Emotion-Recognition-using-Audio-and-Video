# Emotion-Recognition-using-Audio-and-Video
Recruitment Project for 2nd Year


The sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc.
This shape determines what sound comes out. If we can determine the shape accurately, this should give us
an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in
the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this
envelope.

A multilayer perceptron (MLP):
A multilayer perceptron (MLP) is a deep, artificial neural network. It is composed of more than one
perceptron. They are composed of an input layer to receive the signal, an output layer that makes a decision
or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the
true computational engine of the MLP.
<div align="center">
<img width="588" alt="Screenshot 2021-05-21 at 9 03 37 PM" src="https://user-images.githubusercontent.com/57126154/119162491-1240a380-ba78-11eb-8ae0-2c4073859782.png">

</div>
### Loading Dataset
```python
audio = r"C:\Users\HP\Desktop\dataset 2\audio_speech_actors_01-24"
actor_folders = os.listdir(audio) #list files in audio directory
actor_folders.sort() 
actor_folders[0:5]

emotion = []
gender = []
actor = []
file_path = []
for i in actor_folders:
    filename = os.listdir(audio + "\\" +i) #iterate over Actor folders
    for f in filename: # go through files in Actor folder
        part = f.split('.')[0].split('-')
        emotion.append(int(part[2]))
        actor.append(int(part[6]))
        bg = int(part[6])
        if bg%2 == 0:
            bg = "female"
        else:
            bg = "male"
        gender.append(bg)
        file_path.append(audio + "\\" + i + "\\" + f)
```

Then we convert our data into data frame and then extract mfcc features , you can refer code for more info.

## Model 
```python

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, load_model
from keras.layers import *
from keras.optimizers import RMSprop

#model = Sequential()
# First GRU layer with Dropout regularisation
#model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))
#model.add(Dropout(0.2))
# Second GRU layer
#model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))
#model.add(Dropout(0.2))
# Third GRU layer
#model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))
#model.add(Dropout(0.2))
# Fourth GRU layer
#model.add(GRU(units=50, activation='tanh'))
#model.add(Dropout(0.2))
# The output layer
#model.add(Dense(units=8))

#model = Sequential()
#model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))
#model.add(Dense(64))
#model.add(Dropout(0.4))
#model.add(Activation('relu'))
#model.add(Dense(32))
#model.add(Dropout(0.4))
#model.add(Activation('relu'))
#model.add(Dense(8))
#model.add(Activation('softmax'))

#BUILD 1D CNN LAYERS
model = tf.keras.Sequential()
model.add(layers.Conv1D(64, kernel_size=(10), activation='relu', input_shape=(X_train.shape[1],1)))
model.add(layers.Conv1D(128, kernel_size=(10),activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))
model.add(layers.MaxPooling1D(pool_size=(8)))
model.add(layers.Dropout(0.4))
model.add(layers.Conv1D(128, kernel_size=(10),activation='relu'))
model.add(layers.MaxPooling1D(pool_size=(8)))
model.add(layers.Dropout(0.4))
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dropout(0.4))
model.add(layers.Dense(8, activation='sigmoid'))
opt = keras.optimizers.Adam(lr=0.001)
model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()
```
```python
import tensorflow.keras as keras

# FIT MODEL AND USE CHECKPOINT TO SAVE BEST MODEL
checkpoint = ModelCheckpoint("best_initial_model.hdf5", monitor='val_accuracy', verbose=1,
    save_best_only=True, mode='max', period=1, save_weights_only=True)

model_history=model.fit(X_train, y_train,batch_size=32, epochs=50, validation_data=(X_test, y_test),callbacks=[checkpoint])
```
<div align="center">
<img width="644" alt="Screenshot 2021-05-21 at 8 37 04 PM" src="https://user-images.githubusercontent.com/57126154/119161914-6bf49e00-ba77-11eb-9ec0-3f9870009f43.png">
</div>
First we tried it using RNN but accuracy was not that much that (RNN model is commented) and rest is CNN part which gives decent accuracy.

### Printing Confusion Matrix 
```python
cm = confusion_matrix(actual, predictions)
plt.figure(figsize = (12, 10))
cm = pd.DataFrame(cm , index = [i for i in lb.classes_] , columns = [i for i in lb.classes_])
ax = sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5)
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.savefig('Initial_Model_Confusion_Matrix.png')
plt.show()
```
<div align="center">
<img width="672" alt="Screenshot 2021-05-21 at 8 35 33 PM" src="https://user-images.githubusercontent.com/57126154/119160467-f0deb800-ba75-11eb-8e20-ad84cab7de52.png">
</div>
## Print classification report:
```python
print(classification_report(actual, predictions, target_names = ['angry','calm','disgust','fear','happy','neutral','sad','surprise']))
```
After 50 epochs we got accuracy as 62.53% and validation accuracy as 54.51%.
